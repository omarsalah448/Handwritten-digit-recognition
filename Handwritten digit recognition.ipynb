{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a07972a",
   "metadata": {},
   "source": [
    "## Handwritten digit recognition using mnist dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9158f2e1",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04078aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e72ae",
   "metadata": {},
   "source": [
    "### Loading and exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70671f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1d483a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set X = (60000, 28, 28) , Y = (60000,)\n",
      "Shape of testing set  X = (10000, 28, 28) , Y = (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of training set X = {} , Y = {}'.format(x_train.shape, y_train.shape))\n",
    "print('Shape of testing set  X = {} , Y = {}'.format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98e9d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the dataset\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87f342b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set X = (60000, 28, 28, 1) , Y = (60000, 10)\n",
      "Shape of testing set  X = (10000, 28, 28, 1) , Y = (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of training set X = {} , Y = {}'.format(x_train.shape, y_train.shape))\n",
    "print('Shape of testing set  X = {} , Y = {}'.format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee8e101",
   "metadata": {},
   "source": [
    "### Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e29c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdd2d9d",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "954f509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), activation = 'relu', input_shape = (28, 28, 1)))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 128, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units = 64, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#units = 10 as we have 10 digits 0-9\n",
    "model.add(Dense(units = 10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40844a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 37s 71ms/step - loss: 0.3998 - accuracy: 0.8756 - val_loss: 0.0660 - val_accuracy: 0.9806\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 32s 69ms/step - loss: 0.1144 - accuracy: 0.9694 - val_loss: 0.0451 - val_accuracy: 0.9852\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 33s 70ms/step - loss: 0.0800 - accuracy: 0.9793 - val_loss: 0.0355 - val_accuracy: 0.9893\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 32s 69ms/step - loss: 0.0623 - accuracy: 0.9836 - val_loss: 0.0321 - val_accuracy: 0.9909\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 33s 70ms/step - loss: 0.0532 - accuracy: 0.9861 - val_loss: 0.0370 - val_accuracy: 0.9898\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 33s 70ms/step - loss: 0.0473 - accuracy: 0.9880 - val_loss: 0.0293 - val_accuracy: 0.9912\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 33s 70ms/step - loss: 0.0406 - accuracy: 0.9898 - val_loss: 0.0251 - val_accuracy: 0.9928\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 33s 70ms/step - loss: 0.0339 - accuracy: 0.9910 - val_loss: 0.0303 - val_accuracy: 0.9920\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 33s 69ms/step - loss: 0.0311 - accuracy: 0.9916 - val_loss: 0.0269 - val_accuracy: 0.9936\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 32s 69ms/step - loss: 0.0282 - accuracy: 0.9924 - val_loss: 0.0293 - val_accuracy: 0.9931\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.0293 - accuracy: 0.9931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02926185354590416, 0.9930999875068665]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = keras.optimizers.Adam(learning_rate = 0.001),\n",
    "              loss = keras.losses.categorical_crossentropy, metrics = ['accuracy'])\n",
    "fit = model.fit(x = x_train, y = y_train, validation_data = (x_test, y_test),batch_size = 128, epochs = 10, shuffle = True)\n",
    "model.evaluate(x = x_test, y = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "423b08a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted number: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_image = \"num3.png\"  # feel free to change the image\n",
    "\n",
    "# preprocesing the image to fit the algorithm.\n",
    "image = np.array(Image.open(my_image).resize((28,28)))\n",
    "image = image[:,:,0]\n",
    "plt.imshow(image);\n",
    "image = image / 255.\n",
    "image = image.reshape((1,28,28,1))\n",
    "my_predicted_image = model.predict(image)\n",
    "print('Predicted number: {}'.format(np.argmax(my_predicted_image)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61a0480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
